{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import *\n",
    "import torchvision.transforms as transforms\n",
    "from utils import *\n",
    "from train import evaluate, evaluate_plus\n",
    "from utils_compress import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy, os\n",
    "torch.manual_seed(808)\n",
    "random.seed(909)\n",
    "numpy.random.seed(303)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Model and Dataset configuration\n",
    "parser.add_argument('--dataset', type=str, default='UVG', help='dataset')\n",
    "parser.add_argument('--model_type', type=str, default='D-NeRV', choices=['NeRV', 'D-NeRV', 'HDNeRV','HDNeRV2', 'HDNeRV3','RAFT', 'RAFT_t'])\n",
    "parser.add_argument('--model_size', type=str, default='S', choices=['XS', 'S', 'M', 'L', 'XL'])\n",
    "parser.add_argument('--embed', type=str, default='1.25_240', help='base value/embed length for position encoding')\n",
    "parser.add_argument('--spatial_size_h', type=int, default=256)\n",
    "parser.add_argument('--spatial_size_w', type=int, default=320)\n",
    "parser.add_argument('--keyframe_quality', type=int, default=3, help='keyframe quality, control flag used for keyframe image compression')\n",
    "parser.add_argument('--clip_size', type=int, default=8, help='clip_size to sample at a single time')\n",
    "parser.add_argument('--fc_hw', type=str, default='4_5', help='out hxw size for mlp')\n",
    "parser.add_argument('--fc_dim', type=str, default='100', help='out channel size for mlp')\n",
    "parser.add_argument('--enc_dim', type=int, nargs='+', default=[80, 160, 320, 640], help='enc latent dim and embedding ratio')\n",
    "parser.add_argument('--enc_block', type=int, nargs='+', default=[3, 3, 9, 3, 3], help='blocks list')\n",
    "parser.add_argument('--expansion', type=float, default=2, help='channel expansion from fc to conv')\n",
    "parser.add_argument('--strides', type=int, nargs='+', default=[4, 2, 2, 2, 2], help='strides list')\n",
    "parser.add_argument('--lower_width', type=int, default=32, help='lowest channel width for output feature maps')\n",
    "parser.add_argument('--ver', action='store_true', default=True, help='ConvNeXt Version')\n",
    "parser.add_argument('--ignore', action='store_true', default=False, help='Ignore image')\n",
    "\n",
    "# General training setups\n",
    "parser.add_argument('-j', '--workers', type=int, help='number of data loading workers', default=16)\n",
    "parser.add_argument('-b', '--batchSize', type=int, default=8, help='input batch size')\n",
    "parser.add_argument('-e', '--epochs', type=int, default=400, help='number of epochs to train for')\n",
    "parser.add_argument('--warmup', type=float, default=0.2, help='warmup epoch ratio compared to the epochs, default=0.2')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--lr_type', type=str, default='cos', help='learning rate type, default=cos')\n",
    "parser.add_argument('--loss_type', type=str, default='Fusion6', help='loss type, default=L2')\n",
    "parser.add_argument('--start_epoch', type=int, default=0, help='starting epoch')\n",
    "\n",
    "# evaluation parameters\n",
    "parser.add_argument('--weight', default='None', type=str, help='pretrained weights for ininitialization')\n",
    "parser.add_argument('--eval_only', action='store_true', default=False, help='do evaluation only')\n",
    "parser.add_argument('--quant_model', action='store_true', default=False, help='apply model quantization from torch.float32 to torch.int8')\n",
    "parser.add_argument('--quant_model_bit', type=int, default=8, help='bit length for model quantization, default int8')\n",
    "parser.add_argument('--quant_axis', type=int, default=1, help='quantization axis (1 for D-NeRV, 0 for NeRV)')\n",
    "parser.add_argument('--dump_images', action='store_true', default=False, help='dump the prediction images')\n",
    "\n",
    "# distribute learning parameters\n",
    "parser.add_argument('--seed', type=int, default=1, help='manual seed')\n",
    "parser.add_argument('--init_method', default='tcp://127.0.0.1:9888', type=str, help='url used to set up distributed training')\n",
    "parser.add_argument('-d', '--distributed', action='store_true', default=False, help='distributed training')\n",
    "\n",
    "parser.add_argument('-p', '--print-freq', default=500, type=int,)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_mean = [0.4519, 0.4505, 0.4519]\n",
    "args.dataset_std = [0.2434, 0.2547, 0.2958]\n",
    "args.clip_size=8\n",
    "args.strides=[4, 2, 2, 2, 2]\n",
    "args.lower_width=32\n",
    "args.fc_hw='4_5'\n",
    "args.enc_dim=[80, 80, 80, 80, 40]\n",
    "args.enc_block=[3, 3, 9, 3, 3]\n",
    "args.fc_dim=args.enc_dim[-1]\n",
    "args.expansion=2\n",
    "args.outf='out_compress/hdnerv3'\n",
    "args.model_type='HDNeRV3'\n",
    "PE = PositionalEncoding('1.25_240')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import HDNeRV3\n",
    "model = HDNeRV3(fc_hw=args.fc_hw, enc_dim=args.enc_dim,enc_block=args.enc_block, fc_dim=args.fc_dim, expansion=args.expansion, \n",
    "                        stride_list=args.strides, lower_width=args.lower_width, \n",
    "                        clip_size=args.clip_size, device=device,\n",
    "                        dataset_mean=args.dataset_mean, dataset_std=args.dataset_std, ver=args.ver).to(device)\n",
    "\n",
    "transform_rgb = transforms.Compose([transforms.ToTensor()])\n",
    "transform_keyframe = transforms.Compose([transforms.ToTensor(), transforms.Normalize(args.dataset_mean, args.dataset_std)])\n",
    "\n",
    "val_dataset = Dataset_DNeRV_UVG(args, transform_rgb, transform_keyframe)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batchSize, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True, drop_last=False, worker_init_fn=worker_init_fn, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Lấy danh sách các khóa từ model\n",
    "model_keys = list(model.state_dict().keys())\n",
    "\n",
    "# Tên tệp YAML bạn muốn tạo\n",
    "yaml_file = 'hdnerv3.yaml'\n",
    "\n",
    "# Ghi danh sách khóa vào tệp YAML\n",
    "with open(yaml_file, 'w') as f:\n",
    "    yaml.dump(model_keys, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video, input_index, keyframe, backward_distance, frame_mask = next(iter(val_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video, input_index, keyframe, backward_distance, frame_mask = video.to(device), input_index.to(device), keyframe.to(device), backward_distance.to(device), frame_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='checkpoints/HDNeRV3/S.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "orig_ckt = checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(orig_ckt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1464it [02:33,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization\n",
      "Huffman Encoding\n",
      "Dequantization\n",
      "BPP:  0.04133354028065999\n",
      "Rank:0, Step [1/1464], PSNR: 33.2, MSSSIM: 0.8627\n",
      "Rank:0, Step [501/1464], PSNR: 35.43, MSSSIM: 0.9406\n",
      "Rank:0, Step [1001/1464], PSNR: 35.47, MSSSIM: 0.9538\n"
     ]
    }
   ],
   "source": [
    "psnr1,ms1,bpp1 = evaluate_plus(model, val_dataloader, local_rank=0, args=args, method ='normal',length_dataset=len(val_dataset), frame_path_list=val_dataset.frame_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [00:31,  6.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1464it [03:38,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZE APPROXIMATOR AND ENCODER...DONE in 0.0063 s\n",
      "APPROXIMATING WITH METHOD uniform..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE in 4.5990 s\n",
      "ENCODING...DONE in 115.8408 s\n",
      "COMPRESSED FROM 224870400 BYTES TO 64618126 BYTES (64618.13 KB, 64.62 MB, COMPRESSION RATIO: 28.74 %) in 120.4472 s\n",
      "DECODING...DONE in 27.6450 s\n",
      "RECONSTRUCTING...DONE in 0.2619 s\n",
      "INITIALIZE APPROXIMATOR AND ENCODER...DONE in 0.0011 s\n",
      "APPROXIMATING WITH METHOD uniform...DONE in 0.1391 s\n",
      "ENCODING...DONE in 1.9404 s\n",
      "COMPRESSED FROM 6691340 BYTES TO 1583435 BYTES (1583.43 KB, 1.58 MB, COMPRESSION RATIO: 23.66 %) in 2.0817 s\n",
      "DECODING...DONE in 0.3788 s\n",
      "RECONSTRUCTING...DONE in 0.0091 s\n",
      "BPP:  0.06899970320404553\n",
      "Rank:0, Step [1/1464], PSNR: 33.2, MSSSIM: 0.8627\n",
      "Rank:0, Step [501/1464], PSNR: 35.41, MSSSIM: 0.9405\n",
      "Rank:0, Step [1001/1464], PSNR: 35.58, MSSSIM: 0.9541\n"
     ]
    }
   ],
   "source": [
    "psnr2,ms2,bpp2 = evaluate_plus(model, val_dataloader, local_rank=0, args=args, method ='cabac',length_dataset=len(val_dataset), frame_path_list=val_dataset.frame_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -30 --> -46, step = 4\n",
    "# bitstream = nnc.compress_model(model, qp=-44, return_bitstream=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hungvv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
